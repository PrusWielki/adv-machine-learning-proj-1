{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate training data\n",
    "    1. n =1000, 2 features, binary variable from Bernoulli distribution with p=0.5, features for class 0 generated independently from a normal distribution (0,1), the second (a,1). Create such datasets for many different a's\n",
    "    2. n = 1000, 2 features, 2 features, Bernoulli 0.5, 2d normal dist (0,1,p), 2d normal dist (a,1,p)\n",
    "2. Compare, for a =0.1,0.5,1,2,3,5, p=0.5, different train/test splits, boxplots of accuracy for each method and each parameter a.\n",
    "3. Compare, a=2, p=0,0.1,0.3,0.5,0.7,0.9, different train/test splits, boxplots each method each p\n",
    "4. For a=2, p = 0.5, scatter plot training set, mark observations belonging to different classes with colors and draw curves for LDA and QDA.\n",
    "5. Real Data: 3 datasets from the link, binary classification\n",
    "    1. compare lda, qda, nb. split data into a training set and a test set. train the model on trains et and compute accuracy on the test set, repeat for a different train, test splits, generate boxplots for each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulas have been taken from lecture slides.\n",
    "\n",
    "LDA:\n",
    "1. Calculate the assumed common covariance matrix\n",
    "2. Calculate the means for each of the two classes\n",
    "3. Plug into the formula $f_{kl}(x)=wx+b$, when $f(x) < 0$, predicted class: $l$, else predicted class: $k$\n",
    "    1. $w=(\\mu_k-\\mu_l)^T\\Sigma^{-1}$\n",
    "    2. $b=-\\frac{1}{2}(\\mu_k\\mu_l)^T\\Sigma^{-1}(\\mu_k+\\mu_l)+log\\frac{N_k}{N_l}$\n",
    "    \n",
    "QDA:\n",
    "1. Calculate the covariance matrix for each of the two classes\n",
    "2. Calculate the means for each of the two classes\n",
    "3. Plug into the formula $f(x)=sign(w^Tx+b)$\n",
    "    1. $w=\\Sigma^{-1}(\\mu_1-\\mu_{-1})$\n",
    "    2. $b=\\frac{1}{2}(\\mu_{-1}+\\mu_1)^T\\Sigma^{-1}(\\mu_{-1}-\\mu_1)+log\\frac{N_1}{N_{-1}}$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self):\n",
    "        self.w=0\n",
    "        self.b=0\n",
    "        self.mean_0=0\n",
    "        self.mean_1=0\n",
    "        self.covariance_matrix=0\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        classes= np.unique(y)\n",
    "        self.covariance_matrix = np.cov(X[y==classes[0]].T)\n",
    "        self.mean_0=np.mean(X[y==classes[0]], axis=0)\n",
    "        self.mean_1=np.mean(X[y==classes[1]], axis=0)\n",
    "\n",
    "        covariance_matrix_inverse = np.linalg.inv(self.covariance_matrix)\n",
    "        no_0 = (y==classes[0]).sum()\n",
    "        no_1=(y==classes[1]).sum()\n",
    "        # class 0 being class k, and class 1 being class l\n",
    "        self.w=(self.mean_0-self.mean_1).T.dot(covariance_matrix_inverse)\n",
    "        print((self.mean_0+self.mean_1).dot(covariance_matrix_inverse))\n",
    "        self.b=-(1/2)*(self.mean_0-self.mean_1).T.dot(covariance_matrix_inverse).dot((self.mean_0+self.mean_1))+np.log(no_0/no_1)\n",
    "\n",
    "        \n",
    "    def predict_proba(self, Xtest):\n",
    "        f = self.w.dot(Xtest)+self.b\n",
    "        print(\"Predicted: \",f)\n",
    "        return f\n",
    "    def predict(self, Xtest):\n",
    "        f = np.sign(self.w.dot(Xtest)+self.b)\n",
    "        print(\"Predicted class: \",1 if f==-1 else 0)\n",
    "        return 1 if f==-1 else 0\n",
    "    def get_params(self):\n",
    "        print(\"w: \",self.w ,\"\\nb: \",self.b, \"\\nm_0: \",self.mean_0, \"\\nm_1: \",self.mean_1,\"\\ncovariance matrix: \",self.covariance_matrix)\n",
    "        return (self.w, self.b,self.mean_0,self.mean_1,self.covariance_matrix)\n",
    "    \n",
    "class QDA:\n",
    "    def fit(X,y):\n",
    "        return NotImplementedError\n",
    "    def predict_proba(Xtest):\n",
    "        return NotImplementedError\n",
    "    def predict(Xtest):\n",
    "        return NotImplementedError\n",
    "    def get_params():\n",
    "        return NotImplementedError\n",
    "    \n",
    "\n",
    "class NB:\n",
    "    def fit(X,y):\n",
    "        return NotImplementedError\n",
    "    def predict_proba(Xtest):\n",
    "        return NotImplementedError\n",
    "    def predict(Xtest):\n",
    "        return NotImplementedError\n",
    "    def get_params():\n",
    "        return NotImplementedError\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.13179005  1.73842454  7.39926669  1.14517252]\n",
      " [-1.2896735   8.17346849  3.38000997  2.41484807]\n",
      " [ 3.00717123  2.78124415  4.59651867 -0.77841864]\n",
      " [ 6.84612361  4.72044843  1.46683921 -0.98261713]\n",
      " [-0.2974769   2.76191551  3.477662   12.10324243]\n",
      " [ 7.03125447  1.51015117  7.70899129  0.25178899]\n",
      " [ 3.63091488  3.60274858  2.29498817  4.64225082]\n",
      " [ 0.76931606  6.78900579 -4.50943042  0.74917708]\n",
      " [ 2.65709149  4.34121913  3.77477398  7.10477341]\n",
      " [ 2.32667944  3.4772766   3.4585569   0.35840245]] [1 0 1 0 0 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(3, 2.5, size=(10, 4))\n",
    "Y = np.random.randint(2,size=10)\n",
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.6127228   3.01819346  6.35489036 -2.56331025]\n",
      "w:  [ 0.98800239 -0.1979314   0.63877465 -0.81656575] \n",
      "b:  -3.245327417606693 \n",
      "m_0:  [3.60075308 2.94371556 3.8647735  2.1053826 ] \n",
      "m_1:  [2.65684817 3.10882529 3.68790492 1.95839528] \n",
      "covariance matrix:  [[ 6.66601065 -3.90163821 -1.35902893  6.79219403]\n",
      " [-3.90163821  7.90023943 -1.95017669 -7.95912272]\n",
      " [-1.35902893 -1.95017669  4.33756721  2.00490501]\n",
      " [ 6.79219403 -7.95912272  2.00490501 11.53582477]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.98800239, -0.1979314 ,  0.63877465, -0.81656575]),\n",
       " -3.245327417606693,\n",
       " array([3.60075308, 2.94371556, 3.8647735 , 2.1053826 ]),\n",
       " array([2.65684817, 3.10882529, 3.68790492, 1.95839528]),\n",
       " array([[ 6.66601065, -3.90163821, -1.35902893,  6.79219403],\n",
       "        [-3.90163821,  7.90023943, -1.95017669, -7.95912272],\n",
       "        [-1.35902893, -1.95017669,  4.33756721,  2.00490501],\n",
       "        [ 6.79219403, -7.95912272,  2.00490501, 11.53582477]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda=LDA()\n",
    "\n",
    "lda.fit(X,Y)\n",
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheme_1_data_generator(a):\n",
    "    n=1000\n",
    "    bern_prob=0.5\n",
    "    y=np.random.binomial(size=n, n=1, p= bern_prob)\n",
    "    feature_0_0 = np.random.normal(0, 1, size=(1, n)).T\n",
    "    feature_0_1 = np.random.normal(0, 1, size=(1, n)).T\n",
    "    feature_0=np.hstack([feature_0_0,feature_0_1])\n",
    "\n",
    "    feature_1_0 = np.random.normal(a, 1, size=(1, n)).T\n",
    "    feature_1_1 = np.random.normal(a, 1, size=(1, n)).T\n",
    "    feature_1=np.hstack([feature_1_0,feature_1_1])\n",
    "    \n",
    "    X = np.concatenate((feature_0[y==0],feature_1[y==1]))\n",
    "    return (X,y)\n",
    "def scheme_2_data_generator(mean,rho):\n",
    "    n=1000\n",
    "    bern_prob=0.5\n",
    "    y=np.random.binomial(size=n, n=1, p= bern_prob)\n",
    "    # cov(X,Y) = corr(X,Y)*std(X)*std(Y)\n",
    "    features_0 = np.random.multivariate_normal([0,0], [[1,rho],[rho,1]], n)\n",
    "    features_1 = np.random.multivariate_normal([mean,mean], [[1,-rho],[-rho,1]], n)\n",
    "    X = np.concatenate((features_0[y==0],features_1[y==1]))\n",
    "    return (X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.93312921, -0.43047498],\n",
       "        [ 0.31244794,  0.464114  ],\n",
       "        [-0.1503031 ,  0.99612549],\n",
       "        ...,\n",
       "        [ 1.00787493,  1.31566557],\n",
       "        [ 0.45444209,  2.45992746],\n",
       "        [ 1.52140319, -0.2395218 ]]),\n",
       " array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 0, 0]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X,y)=scheme_1_data_generator(2)\n",
    "scheme_2_data_generator(1,0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
