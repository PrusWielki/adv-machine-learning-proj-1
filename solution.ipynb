{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulas have been taken from lecture slides.\n",
    "\n",
    "LDA:\n",
    "1. Calculate the assumed common covariance matrix\n",
    "2. Calculate the means for each of the two classes\n",
    "3. Plug into the formula $f_{kl}(x)=wx+b$, when $f(x) < 0$, predicted class: $l$, else predicted class: $k$\n",
    "    1. $w=(\\mu_k-\\mu_l)^T\\Sigma^{-1}$\n",
    "    2. $b=-\\frac{1}{2}(\\mu_k\\mu_l)^T\\Sigma^{-1}(\\mu_k+\\mu_l)+log\\frac{N_k}{N_l}$\n",
    "    \n",
    "QDA:\n",
    "1. Calculate the covariance matrix for each of the two classes\n",
    "2. Calculate the means for each of the two classes\n",
    "3. Plug into the formula $f(x)=sign(w^Tx+b)$\n",
    "    1. $w=\\Sigma^{-1}(\\mu_1-\\mu_{-1})$\n",
    "    2. $b=\\frac{1}{2}(\\mu_{-1}+\\mu_1)^T\\Sigma^{-1}(\\mu_{-1}-\\mu_1)+log\\frac{N_1}{N_{-1}}$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self):\n",
    "        self.w=0\n",
    "        self.b=0\n",
    "        self.mean_0=0\n",
    "        self.mean_1=0\n",
    "        self.covariance_matrix=0\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        classes= np.unique(y)\n",
    "        self.covariance_matrix = np.cov(X[y==classes[0]].T)\n",
    "        self.mean_0=np.mean(X[y==classes[0]], axis=0)\n",
    "        self.mean_1=np.mean(X[y==classes[1]], axis=0)\n",
    "\n",
    "        covariance_matrix_inverse = np.linalg.inv(self.covariance_matrix)\n",
    "        no_0 = (y==classes[0]).sum()\n",
    "        no_1=(y==classes[1]).sum()\n",
    "        # class 0 being class k, and class 1 being class l\n",
    "        self.w=(self.mean_0-self.mean_1).T.dot(covariance_matrix_inverse)\n",
    "        self.b=-(1/2)*(self.mean_0-self.mean_1).T.dot(covariance_matrix_inverse).dot((self.mean_0+self.mean_1))+np.log(no_0/no_1)\n",
    "\n",
    "        \n",
    "    def predict_proba(self, Xtest):\n",
    "        f = self.w.dot(Xtest)+self.b\n",
    "        # print(\"Predicted: \",f)\n",
    "        return f\n",
    "    def predict(self, Xtest):\n",
    "        f = np.sign(self.w.dot(Xtest)+self.b)\n",
    "        # print(\"Predicted class: \",1 if f==-1 else 0)\n",
    "        return 1 if f==-1 else 0\n",
    "    def get_params(self):\n",
    "        print(\"w: \",self.w ,\"\\nb: \",self.b, \"\\nm_0: \",self.mean_0, \"\\nm_1: \",self.mean_1,\"\\ncovariance matrix: \",self.covariance_matrix)\n",
    "        return (self.w, self.b,self.mean_0,self.mean_1,self.covariance_matrix)\n",
    "    \n",
    "class QDA:\n",
    "    def fit(X,y):\n",
    "        return NotImplementedError\n",
    "    def predict_proba(Xtest):\n",
    "        return NotImplementedError\n",
    "    def predict(Xtest):\n",
    "        return NotImplementedError\n",
    "    def get_params():\n",
    "        return NotImplementedError\n",
    "    \n",
    "\n",
    "class NB:\n",
    "    def fit(X,y):\n",
    "        return NotImplementedError\n",
    "    def predict_proba(Xtest):\n",
    "        return NotImplementedError\n",
    "    def predict(Xtest):\n",
    "        return NotImplementedError\n",
    "    def get_params():\n",
    "        return NotImplementedError\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheme_1_data_generator(a):\n",
    "    n=1000\n",
    "    bern_prob=0.5\n",
    "    y=np.random.binomial(size=n, n=1, p= bern_prob)\n",
    "    feature_0_0 = np.random.normal(0, 1, size=(1, n)).T\n",
    "    feature_0_1 = np.random.normal(0, 1, size=(1, n)).T\n",
    "    feature_0=np.hstack([feature_0_0,feature_0_1])\n",
    "\n",
    "    feature_1_0 = np.random.normal(a, 1, size=(1, n)).T\n",
    "    feature_1_1 = np.random.normal(a, 1, size=(1, n)).T\n",
    "    feature_1=np.hstack([feature_1_0,feature_1_1])\n",
    "    \n",
    "    X = np.concatenate((feature_0[y==0],feature_1[y==1]))\n",
    "    return (X,y)\n",
    "def scheme_2_data_generator(mean,rho):\n",
    "    n=1000\n",
    "    bern_prob=0.5\n",
    "    y=np.random.binomial(size=n, n=1, p= bern_prob)\n",
    "    # cov(X,Y) = corr(X,Y)*std(X)*std(Y)\n",
    "    features_0 = np.random.multivariate_normal([0,0], [[1,rho],[rho,1]], n)\n",
    "    features_1 = np.random.multivariate_normal([mean,mean], [[1,-rho],[-rho,1]], n)\n",
    "    X = np.concatenate((features_0[y==0],features_1[y==1]))\n",
    "    return (X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model:LDA|QDA|NB,X_test,y_test):\n",
    "    correct=0\n",
    "    index =0\n",
    "    for x in X_test:\n",
    "        result=model.predict(x)\n",
    "        if result==y_test[index]:\n",
    "            correct=correct + 1\n",
    "        index = index + 1\n",
    "    return correct/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.5166666666666667\n",
      "0.45666666666666667\n",
      "0.4866666666666667\n",
      "0.49333333333333335\n",
      "0.5033333333333333\n",
      "0.462\n",
      "0.504\n",
      "0.516\n",
      "0.52\n",
      "0.512\n",
      "0.47\n",
      "0.49\n",
      "0.52\n",
      "0.49714285714285716\n",
      "0.51\n",
      "0.49714285714285716\n",
      "0.4857142857142857\n",
      "0.4888888888888889\n",
      "0.5\n",
      "0.4911111111111111\n",
      "0.5055555555555555\n",
      "0.5044444444444445\n",
      "0.51\n"
     ]
    }
   ],
   "source": [
    "(X,y)=scheme_1_data_generator(2)\n",
    "lda=LDA()\n",
    "# scheme_2_data_generator(1,0.5)\n",
    "splits = [0.3,0.5,0.7,0.9]\n",
    "a_params = [0.1,0.5,1,2,3,5]\n",
    "for x in splits:\n",
    "    for a in a_params:\n",
    "        (X,y)=scheme_1_data_generator(a)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=x, random_state=42)\n",
    "        lda.fit(X_train,y_train)\n",
    "        print(get_accuracy(lda,X_test,y_test))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
